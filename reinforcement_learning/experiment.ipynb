{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Notes\n",
    "-------\n",
    "\n",
    "\n",
    "# Data Structure\n",
    "    - Look at unit tests for Data Class.\n",
    "    - Consider dataframes > dictionaries for reasonability.\n",
    "    \n",
    "# RL models\n",
    "    - logical struct.\n",
    "    -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "\n",
    "import numpy as np\n",
    "import statistics\n",
    "import scipy.io\n",
    "import logging\n",
    "import pprint\n",
    "import os\n",
    "from collections import Counter\n",
    "import time\n",
    "import sys\n",
    "import mlflow\n",
    "# internals\n",
    "from src.helpers import log_rat_metadata, log_sequence_data, load_config\n",
    "from src.query_dataset import (QuerySequenceData, load_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.rescorla_wagner_model import (RoscorlaWagner)\n",
    "from src.rescorla_wagner_model_plots import (RescorlaWagnerPlots)\n",
    "from src.rescorla_wagner_model_simulation import (RescorlaWagnerSimulate)\n",
    "from src.rescorla_wagner_model_diagnostics import (RoscorlaWagerModelDiagnostics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.cog_sci_random_response_model import (RandomResponseModel)\n",
    "from src.cog_sci_win_stay_lose_shift_model import (WinStayLoseShiftModel)\n",
    "from src.cog_sci_learning_model_base import (MultiArmedBanditModels)\n",
    "from src.cog_sci_roscorla_wagner_model import RoscorlaWagnerModel\n",
    "\n",
    "np.random.seed(2024) # set seed for reproducibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'processed_data': {'location': './data/processed_data/',\n",
       "  'rat_experiment': {'metadata': 'attention_behaviorals_metadata.csv',\n",
       "   'trials': 'attention_behaviorals_actions.pkl'},\n",
       "  'human_pilot_experiment': {'metadata': 'attention_behaviorals_human_pilot_metadata.csv',\n",
       "   'trials': 'attention_behaviorals_human_pilot_actions.pkl'},\n",
       "  'human_motivation_experiment': {'metadata': 'attention_behaviorals_human_motivation_metadata.csv',\n",
       "   'trials': 'attention_behaviorals_human_motivation_actions.pkl'}}}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load yaml config\n",
    "_config = load_config('config.yaml')\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "_config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "# Experiment Tracker\n",
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tracking_Experiments:\n",
    "\n",
    "    def __init__(self) -> None:\n",
    "        self._results = self.init_results()\n",
    "\n",
    "    def update_results(self, results):\n",
    "        for key, val in results.items():\n",
    "            \n",
    "            # check if key is valid\n",
    "            if key not in self._results.keys():\n",
    "                raise ValueError(f\"Key {key} not found in results\")\n",
    "            self._results[key].append(val)\n",
    "        return self\n",
    "\n",
    "    @staticmethod\n",
    "    def init_results():\n",
    "        return {\n",
    "            # describe experiment\n",
    "            'experiment_ID': [],\n",
    "            'experiment': [],\n",
    "\n",
    "            # describe input data\n",
    "            'reward_rate': [],\n",
    "            'action_rate': [],\n",
    "            'corr_action_reward': [],\n",
    "            'corr_stim_resp': [],\n",
    "\n",
    "            'model_1_b_pred': [],\n",
    "            'model_1_negLL': [],\n",
    "            'model_1_BIC': [],\n",
    "\n",
    "            'model_2_epsilon_pred': [],\n",
    "            'model_2_negLL': [],\n",
    "            'model_2_BIC': [],\n",
    "\n",
    "            'model_3_alpha_pred': [],\n",
    "            'model_3_theta_pred': [],\n",
    "            'model_3_negLL': [],\n",
    "            'model_3_BIC': [],\n",
    "            'model_3_opt_init_params': []\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "# Experiment Class\n",
    "----\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up a local directory for MLflow tracking\n",
    "def configure_mlflow_tracking(\n",
    "        mlflow_tracking_dir = \"./mlruns\",\n",
    "        experiment_name = \"RL Experiment - Rat Data - EDS baseline\"):\n",
    "    if not os.path.exists(mlflow_tracking_dir):\n",
    "        os.makedirs(mlflow_tracking_dir)\n",
    "        \n",
    "    print('Warning: using file based method for mlflow tracking.')\n",
    "    mlflow.set_tracking_uri(f\"file://{os.path.abspath(mlflow_tracking_dir)}\")\n",
    "    mlflow.set_experiment(experiment_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# qsd = QuerySequenceData(StimCode, RespCode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def run_experiment_model_1(action_vector, reward_vector, init_guess=[0.5]):\n",
    "    # fit model 1. Random Response Model\n",
    "    rrm = RandomResponseModel()\n",
    "    results = rrm.optimize_scikit(\n",
    "        loss_function=rrm.neg_log_likelihood,\n",
    "        init_guess=init_guess,\n",
    "        args=(action_vector, reward_vector), bounds=[(0,1)])\n",
    "    return results\n",
    "\n",
    "def run_experiment_model_2(action_vector, reward_vector, init_guess=[0.5]):\n",
    "    wsls = WinStayLoseShiftModel()\n",
    "    results = wsls.optimize_scikit(\n",
    "        loss_function=wsls.neg_log_likelihood,\n",
    "        init_guess=init_guess,\n",
    "        args=(action_vector, reward_vector), bounds=[(0,1)])\n",
    "    return results\n",
    "\n",
    "def run_experiment_model_3(action_vector, reward_vector):\n",
    "    rwm = RoscorlaWagnerModel()\n",
    "    results = rwm.optimize_scikit_model_over_init_parameters(\n",
    "        actions=action_vector,\n",
    "        rewards=reward_vector,\n",
    "        loss_function=None,\n",
    "        alpha_init_range=np.linspace(0, 1, 5),\n",
    "        theta_init_range=np.linspace(.1, 10, 7),\n",
    "        bounds=((0,1), (0.1, 10)),\n",
    "        log_progress=False\n",
    "        )\n",
    "    negLL, params_opt, BIC, optimal_init_params = results\n",
    "    return negLL, params_opt, BIC, optimal_init_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_experiment_features(qsd):\n",
    "    _stimCount = Counter(qsd._stimCodeFlat)\n",
    "    _respCount = Counter(qsd._respCodeFlat)\n",
    "    action, reward = qsd._action, qsd._reward\n",
    "    action_mean = np.mean(action)\n",
    "    reward_mean = np.mean(reward)\n",
    "    action_reward_corr = np.corrcoef(action, reward)[0,1]\n",
    "    stim_resp_corr = np.corrcoef(qsd._stimCodeFlat, qsd._respCodeFlat)[0,1]\n",
    "\n",
    "    return _stimCount, _respCount, action, reward, action_mean, reward_mean, action_reward_corr, stim_resp_corr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def generate_experiment(experiments):\n",
    "    for idx, experiment in enumerate(experiments):\n",
    "        yield idx, experiment\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_progress_bar(iteration, total, length=50):\n",
    "    percent = (\"{0:.1f}\").format(100 * (iteration / float(total)))\n",
    "    filled_length = int(length * iteration // total)\n",
    "    bar = 'â–ˆ' * filled_length + '-' * (length - filled_length)\n",
    "    sys.stdout.write(f'\\r|{bar}| {percent}% Complete')\n",
    "    sys.stdout.flush()\n",
    "\n",
    "# # Example loop with manual progress bar\n",
    "# total = 100\n",
    "# for i in range(total):\n",
    "#     time.sleep(0.1)  # Simulate work being done\n",
    "#     print_progress_bar(i + 1, total)\n",
    "\n",
    "# print()  # Move to the next line after the progress bar is complete\n",
    "\n",
    "def generate_experiment(experiments):\n",
    "    for idx, experiment in enumerate(experiments):\n",
    "        yield idx, experiment\n",
    "\n",
    "def run_experiment(idx, exp, StimCode, RespCode, drop_n_trails=0):\n",
    "    # extract experiment data\n",
    "    qsd, action_vector, reward_vector = extract_experiment(exp, StimCode, RespCode)\n",
    "    \n",
    "    # drop n trails\n",
    "    if drop_n_trails > 0:\n",
    "        action_vector = action_vector[drop_n_trails:]\n",
    "        reward_vector = reward_vector[drop_n_trails:]\n",
    "\n",
    "    # generate metadata\n",
    "    _stimCount, _respCount, action, reward, action_mean, reward_mean, action_reward_corr, stim_resp_corr = \\\n",
    "    compute_experiment_features(qsd)\n",
    "\n",
    "    # print(f'exp: {idx}, action rate: {np.mean(action_vector)}, reward rate: {np.mean(reward_vector)}')\n",
    "\n",
    "    # fit experiment 1\n",
    "    result_model_1 = run_experiment_model_1(action_vector, reward_vector)\n",
    "    result_model_2 = run_experiment_model_2(action_vector, reward_vector)\n",
    "    result_model_3 = run_experiment_model_3(action_vector, reward_vector)\n",
    "\n",
    "    # extract model 3 data (not dict)\n",
    "    negLL, params_opt, BIC, optimal_init_params = result_model_3\n",
    "    \n",
    "    del qsd\n",
    "    \n",
    "    # result object\n",
    "    _result = {\n",
    "        # describe experiment\n",
    "        'experiment_ID': idx, 'experiment': exp,\n",
    "\n",
    "        # describe input data\n",
    "        'reward_rate': reward_mean, 'action_rate': action_mean,\n",
    "        'corr_action_reward': action_reward_corr, 'corr_stim_resp': stim_resp_corr,\n",
    "\n",
    "        'model_1_b_pred': result_model_1['param_opt'][0],\n",
    "        'model_1_negLL': result_model_1['negLL'],\n",
    "        'model_1_BIC': result_model_1['BIC'],\n",
    "        'model_2_epsilon_pred': result_model_2['param_opt'][0],\n",
    "        'model_2_negLL': result_model_2['negLL'],\n",
    "        'model_2_BIC': result_model_2['BIC'],\n",
    "        'model_3_alpha_pred': params_opt[0],\n",
    "        'model_3_theta_pred': params_opt[1],\n",
    "        'model_3_negLL': negLL, 'model_3_BIC': BIC,\n",
    "        'model_3_opt_init_params': optimal_init_params\n",
    "    }\n",
    "\n",
    "    return _result       \n",
    "\n",
    "def run_experiment_suite(exp_gen, StimCode, RespCode, n_experiments=100, drop_n_trails=0, mlflow_tracking=True):\n",
    "    results = init_results()\n",
    "\n",
    "    for idx, exp in exp_gen:\n",
    "        # print(f'Experiment: {idx}')\n",
    "        print_progress_bar(idx + 1, n_experiments)\n",
    "\n",
    "        _result = run_experiment(idx, exp, StimCode, RespCode, drop_n_trails=drop_n_trails)\n",
    "        update_results(_result, results) # external store\n",
    "\n",
    "        if mlflow_tracking:\n",
    "            with mlflow.start_run(run_name=f'Experiment {idx}', nested=True):\n",
    "                # extract experiment data\n",
    "                _metrics = {k:v for k,v in _result.items() if isinstance(v, (int, float))}\n",
    "                _params = {k:str(v) for k,v in _result.items() if not isinstance(v, (int, float))}\n",
    "\n",
    "                # Log parameters and metrics to MLFlow\n",
    "                mlflow.log_params(_result)\n",
    "                mlflow.log_metrics(_metrics)\n",
    "\n",
    "                mlflow.log_param('experiment_ID', idx)\n",
    "                mlflow.log_param('experiment', exp)\n",
    "                # [mlflow.log_param(k,v) for k,v in _params.items()]\n",
    "        \n",
    "    print('Runtime complete :).')\n",
    "    return pd.DataFrame(results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_progress_bar(iteration, total, length=50):\n",
    "    percent = (\"{0:.1f}\").format(100 * (iteration / float(total)))\n",
    "    filled_length = int(length * iteration // total)\n",
    "    bar = 'â–ˆ' * filled_length + '-' * (length - filled_length)\n",
    "    sys.stdout.write(f'\\r|{bar}| {percent}% Complete')\n",
    "    sys.stdout.flush()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "expected ':' (295747153.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[5], line 1\u001b[0;36m\u001b[0m\n\u001b[0;31m    class exe\u001b[0m\n\u001b[0m             ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m expected ':'\n"
     ]
    }
   ],
   "source": [
    "\n",
    "class Experiments:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
